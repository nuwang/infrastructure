# Specific settings for galaxy staging application/web server

certbot_domains:
- "{{ hostname }}"

galaxy_db_user_password: "{{ vault_staging_db_user_password }}"

# NFS stuff
nfs_exports:
    - "{{ galaxy_root }}  *(rw,async,no_root_squash,no_subtree_check)"

# ansible-galaxy
# galaxy_repo: https://github.com/galaxyproject/galaxy.git
galaxy_repo: https://github.com/usegalaxy-au/galaxy.git  # usegalaxy-au fork of galaxy
galaxy_commit_id: release_20.09_cloudstor

# Use cloudstor branch and extra tool_conf file and user prefs for cloudstor tool
use_cloudstor_conf: true

galaxy_dynamic_job_rules_src_dir: files/galaxy/dynamic_job_rules/staging
galaxy_dynamic_job_rules_dir: "{{ galaxy_root }}/dynamic_job_rules"
galaxy_dynamic_job_rules:
  - dynamic_rules/destination_mapper.py
  - dynamic_rules/tool_destinations.yml
  - readme.txt

galaxy_tools_indices_dir: "{{ galaxy_root }}"
galaxy_tmp_dir: "{{ galaxy_root }}/tmp"

galaxy_handler_count: 2   ############# europe uses 5, this could be host specific

__galaxy_config:
  uwsgi:
    mule:
      - lib/galaxy/main.py
      - lib/galaxy/main.py
    farm: job-handlers:1,2
  galaxy:
    admin_users: "{{ machine_users | selectattr('email', 'defined') | map(attribute='email') | join(',') }}" # everyone is an admin on staging: # TODO: update this
    brand: "Australia Staging"
    database_connection: "postgres://galaxy:{{ vault_staging_db_user_password }}@staging-db.usegalaxy.org.au:5432/galaxy"
    id_secret: "{{ vault_staging_id_secret }}"
    file_path: "{{ galaxy_root }}/data"
    galaxy_infrastructure_url: 'https://staging.usegalaxy.org.au'

#Galaxy Job Conf
galaxy_jobconf:
    plugin_workers: 4
    handlers:
        count: "{{ galaxy_handler_count }}"
    plugins:
      - id: local
        load: galaxy.jobs.runners.local:LocalJobRunner
        workers: 4
      - id: dynamic
        params:
          rules_module: dynamic_rules
      - id: slurm
        load: galaxy.jobs.runners.slurm:SlurmJobRunner
      - id: pulsar_au_01
        load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
        params:
            amqp_url: "pyamqp://galaxy_au:{{ vault_rabbitmq_password_galaxy_au_staging }}@staging-queue.usegalaxy.org.au:5671//pulsar/galaxy_au?ssl=1"
            galaxy_url: "https://staging.usegalaxy.org.au"
            manager: _default_
            amqp_acknowledge: True
            amqp_ack_republish_time: 300
            amqp_consumer_timeout: 2.0
            amqp_publish_retry: True
            amqp_publish_retry_max_retries: 60
    default_destination: gateway
    destinations:
      - id: local
        runner: local
      - id: gateway
        runner: dynamic
        params:
            type: python
            function: gateway
      - id: dynamic_dtd
        runner: dynamic
        params:
          type: dtd
      - id: slurm_dest
        runner: slurm
        params:
            nativeSpecification: "--nodes=1 --ntasks=2 --ntasks-per-node=2 --mem=7760"
      - id: pulsar_destination
        runner: pulsar_au_01
        params:
              jobs_directory: /mnt/pulsar/files/staging
              transport: curl
              remote_metadata: "false"
              default_file_action: remote_transfer
              dependency_resolution: remote
              rewrite_parameters: "true"
              persistence_directory: /mnt/pulsar/files/persisted_data
              submit_native_specification: "--nodes=1 --ntasks=2 --ntasks-per-node=2"
    limits:
      #General limits for user submission
      - type: anonymous_user_concurrent_jobs
        value: 1
      - type: registered_user_concurrent_jobs
        value: 10

# variables for attaching mounted volume to application server
galaxyserver_attached_volume_device: /dev/vdb
galaxyserver_attached_volume_path: /mnt
galaxyserver_attached_volume_fstype: ext4

# cvmfs
cvmfs_cache_base: /mnt/var/lib/cvmfs

# vars for setting up .pgpass
galaxy_db_password: "{{ vault_staging_db_user_password }}"
reader_db_password: "{{ vault_staging_db_reader_password }}"
db_address: "staging-db.usegalaxy.org.au"
gxadmin_ubuntu_config_dir: /home/ubuntu/.config

# Custos/AAF specific settings
custos_client_id: "{{ vault_custos_client_id_staging }}"
custos_client_secret: "{{ vault_custos_client_secret_staging }}"
aaf_client_id: "{{ vault_aaf_client_id_staging }}"
aaf_client_secret: "{{ vault_aaf_client_secret_staging }}"
